{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './datasets'\n",
    "train_cats_dir = data_dir + '/train/cats/'\n",
    "train_dogs_dir = data_dir + '/train/dogs/'\n",
    "\n",
    "\n",
    "train_tfrecord_file = data_dir + '/train/train.tfrecords'\n",
    "\n",
    "\n",
    "\n",
    "test_cats_dir = data_dir + '/valid/cats/'\n",
    "test_dogs_dir = data_dir + '/valid/dogs/'\n",
    "test_tfrecord_file = data_dir + '/valid/test.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将数据集存储为 TFRecord 文件 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_filenames = [train_cats_dir + filename for filename in os.listdir(train_cats_dir)]\n",
    "train_dog_filenames = [train_dogs_dir + filename for filename in os.listdir(train_dogs_dir)]\n",
    "train_filenames = train_cat_filenames + train_dog_filenames\n",
    "\n",
    "train_labels = [0] * len(train_cat_filenames) + [1] * len(train_dog_filenames)  # 将 cat 类的标签设为0，dog 类的标签设为1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image 文件名字\n",
    "#label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(train_tfrecord_file) as writer:\n",
    "    for filename, label in zip(train_filenames, train_labels):\n",
    "        \n",
    "        image = open(filename, 'rb').read()     # 读取数据集图片到内存，image 为一个 Byte 类型的字符串\n",
    "        \n",
    "        \n",
    "        feature = {                             # 建立 tf.train.Feature 字典\n",
    "            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),  # 图片是一个 Bytes 对象\n",
    "            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))   # 标签是一个 Int 对象\n",
    "        }\n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature)) # 通过字典建立 Example\n",
    "        writer.write(example.SerializeToString())   # 将Example序列化并写入 TFRecord 文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_filenames = [test_cats_dir + filename for filename in os.listdir(test_cats_dir)]\n",
    "test_dog_filenames = [test_dogs_dir + filename for filename in os.listdir(test_dogs_dir)]\n",
    "test_filenames = test_cat_filenames + test_dog_filenames\n",
    "test_labels = [0] * len(test_cat_filenames) + [1] * len(test_dog_filenames)  # 将 cat 类的标签设为0，dog 类的标签设为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(test_tfrecord_file) as writer:\n",
    "    for filename, label in zip(test_filenames, test_labels):\n",
    "        image = open(filename, 'rb').read()     # 读取数据集图片到内存，image 为一个 Byte 类型的字符串\n",
    "        feature = {                             # 建立 tf.train.Feature 字典\n",
    "            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),  # 图片是一个 Bytes 对象\n",
    "            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))   # 标签是一个 Int 对象\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature)) # 通过字典建立 Example\n",
    "        serialized = example.SerializeToString() #将Example序列化\n",
    "        writer.write(serialized)   # 写入 TFRecord 文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取 TFRecord 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_tfrecord_file)    # 读取 TFRecord 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = { # 定义Feature结构，告诉解码器每个Feature的类型是什么\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    \n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_example(example_string): # 将 TFRecord 文件中的每一个序列化的 tf.train.Example 解码\n",
    "    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n",
    "    feature_dict['image'] = tf.io.decode_jpeg(feature_dict['image'])    # 解码JPEG图片\n",
    "    feature_dict['image'] = tf.image.resize(feature_dict['image'], [256, 256]) / 255.0\n",
    "    return feature_dict['image'], feature_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(_parse_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3) tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# for image,label in train_dataset.take(1):\n",
    "#     print(image.shape,label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=23000)    \n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(32,256,256,3) ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.TFRecordDataset(test_tfrecord_file)    # 读取 TFRecord 文件\n",
    "test_dataset = test_dataset.map(_parse_example)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNModel(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.maxpool1 = tf.keras.layers.MaxPooling2D()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 5, activation='relu')\n",
    "        self.maxpool2 = tf.keras.layers.MaxPooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)       \n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = CNNModel()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "#batch\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss) #update\n",
    "    train_accuracy(labels, predictions)#update\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6762517690658569, Accuracy: 62.32608413696289, Test Loss: 0.6289066076278687, Test Accuracy: 64.6500015258789\n",
      "Epoch 2, Loss: 0.5742419958114624, Accuracy: 69.79564666748047, Test Loss: 0.5795500874519348, Test Accuracy: 69.9000015258789\n",
      "Epoch 3, Loss: 0.45443886518478394, Accuracy: 78.77825927734375, Test Loss: 0.5544095635414124, Test Accuracy: 72.69999694824219\n",
      "Epoch 4, Loss: 0.265891432762146, Accuracy: 88.84347534179688, Test Loss: 0.7225342392921448, Test Accuracy: 70.5\n",
      "Epoch 5, Loss: 0.08562982827425003, Accuracy: 97.06086730957031, Test Loss: 1.1647887229919434, Test Accuracy: 69.75\n",
      "Epoch 6, Loss: 0.033000148832798004, Accuracy: 99.06086730957031, Test Loss: 1.614324927330017, Test Accuracy: 70.85000610351562\n",
      "Epoch 7, Loss: 0.021432049572467804, Accuracy: 99.43043518066406, Test Loss: 1.723189115524292, Test Accuracy: 70.25\n",
      "Epoch 8, Loss: 0.01698978617787361, Accuracy: 99.56521606445312, Test Loss: 1.8104292154312134, Test Accuracy: 70.45000457763672\n",
      "Epoch 9, Loss: 0.01910245418548584, Accuracy: 99.44347381591797, Test Loss: 1.9812394380569458, Test Accuracy: 71.05000305175781\n",
      "Epoch 10, Loss: 0.015399742871522903, Accuracy: 99.49130249023438, Test Loss: 2.065403461456299, Test Accuracy: 70.1500015258789\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "for epoch in range(EPOCHS):\n",
    "    # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels) #mini-batch 更新\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100\n",
    "                         ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
