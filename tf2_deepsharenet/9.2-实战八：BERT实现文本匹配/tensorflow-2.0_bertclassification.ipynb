{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import collections\n",
    "import tokenization #\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train_20200228.csv\")\n",
    "val_df = pd.read_csv(\"./data/dev_20200228.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>query1</th>\n",
       "      <th>query2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>咳血</td>\n",
       "      <td>剧烈运动后咯血,是怎么了?</td>\n",
       "      <td>剧烈运动后咯血是什么原因？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>咳血</td>\n",
       "      <td>剧烈运动后咯血,是怎么了?</td>\n",
       "      <td>剧烈运动后为什么会咯血？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>咳血</td>\n",
       "      <td>剧烈运动后咯血,是怎么了?</td>\n",
       "      <td>剧烈运动后咯血，应该怎么处理？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>咳血</td>\n",
       "      <td>剧烈运动后咯血,是怎么了?</td>\n",
       "      <td>剧烈运动后咯血，需要就医吗？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>咳血</td>\n",
       "      <td>剧烈运动后咯血,是怎么了?</td>\n",
       "      <td>剧烈运动后咯血，是否很严重？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id category         query1           query2  label\n",
       "0   0       咳血  剧烈运动后咯血,是怎么了?    剧烈运动后咯血是什么原因？      1\n",
       "1   1       咳血  剧烈运动后咯血,是怎么了?     剧烈运动后为什么会咯血？      1\n",
       "2   2       咳血  剧烈运动后咯血,是怎么了?  剧烈运动后咯血，应该怎么处理？      0\n",
       "3   3       咳血  剧烈运动后咯血,是怎么了?   剧烈运动后咯血，需要就医吗？      0\n",
       "4   4       咳血  剧烈运动后咯血,是怎么了?   剧烈运动后咯血，是否很严重？      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>query1</th>\n",
       "      <th>query2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>咳血</td>\n",
       "      <td>请问呕血与咯血有什么区别？</td>\n",
       "      <td>请问呕血与咯血这两者之间有什么区别？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>咳血</td>\n",
       "      <td>请问呕血与咯血有什么区别？</td>\n",
       "      <td>请问呕血与咯血异同？</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>咳血</td>\n",
       "      <td>请问呕血与咯血有什么区别？</td>\n",
       "      <td>请问呕血与咯血怎么治疗？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>咳血</td>\n",
       "      <td>请问呕血与咯血有什么区别？</td>\n",
       "      <td>请问呕血与咯血是什么原因导致的？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>咳血</td>\n",
       "      <td>请问呕血与咯血有什么区别？</td>\n",
       "      <td>请问呕血与咯血与其他疾病有关联吗？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id category         query1              query2  label\n",
       "0   0       咳血  请问呕血与咯血有什么区别？  请问呕血与咯血这两者之间有什么区别？      1\n",
       "1   1       咳血  请问呕血与咯血有什么区别？          请问呕血与咯血异同？      1\n",
       "2   2       咳血  请问呕血与咯血有什么区别？        请问呕血与咯血怎么治疗？      0\n",
       "3   3       咳血  请问呕血与咯血有什么区别？    请问呕血与咯血是什么原因导致的？      0\n",
       "4   4       咳血  请问呕血与咯血有什么区别？   请问呕血与咯血与其他疾病有关联吗？      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变成bert支持的数据格式\n",
    "- token_id\n",
    "- input_mask \n",
    "- segment_id\n",
    "\n",
    "思考下：\n",
    "\n",
    "query1 和query2 合并成一个句子\n",
    "\n",
    "[cls] query1 [sep] query2 [sep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file= \"./preporcess_data/bert_zh_L-12_H-768_A-12_2/assets/vocab.txt\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.tokenize(\"深度之眼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、句子切割  句子过长需要截断\n",
    "\n",
    "\n",
    "2、两个句子整合一个句子[cls] query1 [sep] query2 [sep]\n",
    "\n",
    "3、句子编码 （segment_ids, input_mask）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3918, 2428, 722, 4706]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self,idx, category, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.id = idx\n",
    "        self.category = category\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    #query1\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    #query 2\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    #截断\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "    # [cls] [query1] [sep] [query2] [sep]\n",
    "    # 0       0       0     1       1\n",
    "        \n",
    "        \n",
    "    #句子编码\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    label_id = label_map[example.label]\n",
    "    #类记录一条问题，也可以用字典\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(output_file) # \n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example, label_list,\n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
    "\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _create_examples(lines,set_type='train'):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "\n",
    "        idx = line[0]\n",
    "        category = tokenization.convert_to_unicode(line[1])\n",
    "        text_a = tokenization.convert_to_unicode(line[2])\n",
    "        text_b = tokenization.convert_to_unicode(line[3])\n",
    "        if set_type == \"test\":\n",
    "            label = 0\n",
    "        else:\n",
    "            label = line[4]\n",
    "        examples.append(\n",
    "            InputExample(idx=idx,category=category, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = _create_examples(train_df.values,set_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_based_convert_examples_to_features(Examples,label_list=[0,1], max_seq_length=40, tokenizer=tokenizer, output_file=\"./preporcess_data/train.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = convert_single_example(0, Examples[0], label_list=[0,1], max_seq_length=40, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length= 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    'input_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "    'input_mask': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "    'segment_ids': tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "    'label_ids': tf.io.FixedLenFeature([1], tf.int64),\n",
    "}\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return {\"input_ids\":example['input_ids'],\"input_mask\":example['input_mask'],\"segment_ids\":example['segment_ids']},example['label_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.TFRecordDataset(\"./preporcess_data/train.tfrecord\")\n",
    "train_ds = train_ds.map(\n",
    "    lambda record: decode_record(record, name_to_features),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat().batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\n",
      "array([[ 101, 1196, 4164, ...,    0,    0,    0],\n",
      "       [ 101, 1196, 4164, ...,    0,    0,    0],\n",
      "       [ 101, 1196, 4164, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 2769, 1744, ...,    0,    0,    0],\n",
      "       [ 101, 1920, 1492, ...,    0,    0,    0],\n",
      "       [ 101, 1920, 1492, ...,    0,    0,    0]])>, 'input_mask': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>, 'segment_ids': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>}, <tf.Tensor: shape=(32, 1), dtype=int32, numpy=\n",
      "array([[1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1]])>)\n"
     ]
    }
   ],
   "source": [
    "for line in train_ds:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model(num_labels,\n",
    "                     max_seq_length=None,\n",
    "                     final_layer_initializer=None,\n",
    "                     hub_module_url=None,\n",
    "                     hub_module_trainable=True):\n",
    "    \"\"\"BERT classifier model in functional API style.\n",
    "\n",
    "    Construct a Keras model for predicting `num_labels` outputs from an input with\n",
    "    maximum sequence length `max_seq_length`.\n",
    "\n",
    "    Args:\n",
    "      bert_config: BertConfig or AlbertConfig, the config defines the core BERT or\n",
    "        ALBERT model.\n",
    "      num_labels: integer, the number of classes.\n",
    "      max_seq_length: integer, the maximum input sequence length.\n",
    "      final_layer_initializer: Initializer for final dense layer. Defaulted\n",
    "        TruncatedNormal initializer.\n",
    "      hub_module_url: TF-Hub path/url to Bert module.\n",
    "      hub_module_trainable: True to finetune layers in the hub module.\n",
    "\n",
    "    Returns:\n",
    "      Combined prediction model (words, mask, type) -> (one-hot labels)\n",
    "      BERT sub-model (words, mask, type) -> (bert_outputs)\n",
    "    \"\"\"\n",
    "    if final_layer_initializer is not None:\n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,), dtype=tf.int32, name='input_ids')\n",
    "    input_mask = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "    input_type_ids = tf.keras.layers.Input(\n",
    "        shape=(max_seq_length,), dtype=tf.int32, name='segment_ids')\n",
    "    \n",
    "    bert_model = hub.KerasLayer(hub_module_url, trainable=hub_module_trainable)  #重点\n",
    "    \n",
    "    \n",
    "    pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids]) #[cls] [sequence ] #768\n",
    "    output = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n",
    "\n",
    "    output = tf.keras.layers.Dense(num_labels, kernel_initializer=initializer, name='output')(output)\n",
    "    return tf.keras.Model(\n",
    "        inputs={\n",
    "            'input_ids': input_word_ids,\n",
    "            'input_mask': input_mask,\n",
    "            'segment_ids': input_type_ids\n",
    "        },\n",
    "        outputs=output), bert_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "model, core_model = classifier_model(num_labels=2,max_seq_length=40,hub_module_url='./preporcess_data/bert_zh_L-12_H-768_A-12_2',hub_module_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 102267649   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            1538        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 102,269,187\n",
      "Trainable params: 102,269,186\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflow==2.1.0  2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 274 steps\n",
      "Epoch 1/10\n",
      "107/274 [==========>...................] - ETA: 16:58 - loss: 0.4573 - accuracy: 0.7754"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds,epochs=10,steps_per_epoch=274)\n",
    "#0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有个验证集\n",
    "#版本问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model,\"./save_models/bert_version1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_saved_model = tf.saved_model.load(\"./save_models/bert_version1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = restored_saved_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = convert_single_example(0, Examples[0], label_list=[0,1], max_seq_length=40, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-03 16:12:27.666551: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-07-03 16:12:27.666676: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2020-07-03 16:12:27.666696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 40)\n",
      "        name: serving_default_input_ids:0\n",
      "    inputs['input_mask'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 40)\n",
      "        name: serving_default_input_mask:0\n",
      "    inputs['segment_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 40)\n",
      "        name: serving_default_segment_ids:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 2)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /opt/conda/envs/Python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='segment_ids')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/2')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/2')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='segment_ids')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='segment_ids')]\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='segment_ids')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='segment_ids')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/2')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/1'), TensorSpec(shape=(None, 40), dtype=tf.int32, name='inputs/2')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir ./save_models/bert_version1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = test_sample.input_ids\n",
    "input_mask = test_sample.input_mask\n",
    "segment_ids = test_sample.segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[-3.4082842,  2.4501426],\n",
       "        [-3.4082842,  2.4501426]], dtype=float32)>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(input_ids=tf.constant([input_ids,input_ids]),input_mask=tf.constant([input_mask,input_mask]),segment_ids=tf.constant([segment_ids,segment_ids]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
