{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例1、 keras版本模型训练\n",
    "相关函数\n",
    "- 构建模型（顺序模型、函数式模型、子类模型）\n",
    "- 模型训练：model.fit()\n",
    "- 模型验证：model.evaluate()\n",
    "- 模型预测： model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32,))  #(batch_size=32,数据维度32)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(inputs) #（64个神经元，）\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)#（63个神经元）\n",
    "predictions = tf.keras.layers.Dense(10)(x) #（输出是10类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- inputs(模型输入)\n",
    "#- output(模型输出)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "#指定损失函数 (loss) tf.keras.optimizers.RMSprop\n",
    "#优化器 (optimizer) tf.keras.losses.SparseCategoricalCrossentropy\n",
    "#指标 (metrics) ['accuracy'] \n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), #优化器\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), #损失函数\n",
    "              metrics=['accuracy']) #评估函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提供[许多内置的优化器，损失和指标](https://www.tensorflow.org/guide/keras/train_and_evaluate#many_built-in_optimizers_losses_and_metrics_are_available)\n",
    "通常，不必从头开始创建自己的损失，指标或优化函数，因为所需的可能已经是Keras API的一部分：\n",
    "\n",
    "优化器：\n",
    "- SGD() （有或没有动量）\n",
    "- RMSprop()\n",
    "- Adam()\n",
    "\n",
    "损失：\n",
    "- MeanSquaredError()\n",
    "- KLDivergence()\n",
    "- CosineSimilarity()\n",
    "\n",
    "指标：\n",
    "- AUC()\n",
    "- Precision()\n",
    "- Recall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，如果想用上述的默认设置，那么在很多情况下，可以通过字符串标识符指定优化器，损失和指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###构建数据集\n",
    "#\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 32))\n",
    "y_train = np.random.randint(10, size=(1000, ))\n",
    "\n",
    "x_val = np.random.random((200, 32))\n",
    "y_val = np.random.randint(10, size=(200, ))\n",
    "\n",
    "x_test = np.random.random((200, 32))\n",
    "y_test = np.random.randint(10, size=(200, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过将数据切成大小为“ batch_size”的“批”来训练模型，并针对给定数量的“epoch”重复遍历整个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model.fit)\n",
    "#N/batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.3355 - accuracy: 0.1050 - val_loss: 2.3096 - val_accuracy: 0.1200\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.3024 - accuracy: 0.1110 - val_loss: 2.3152 - val_accuracy: 0.1050\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2910 - accuracy: 0.1310 - val_loss: 2.3038 - val_accuracy: 0.1200\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2802 - accuracy: 0.1510 - val_loss: 2.3107 - val_accuracy: 0.1100\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2703 - accuracy: 0.1540 - val_loss: 2.3136 - val_accuracy: 0.1100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba36cc9350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=  (x_val, y_val)     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动划分验证集\n",
    "\n",
    "在前面的例子中，我们使用validation_data参数将Numpy数组的元组传递(x_val, y_val)给模型，以在每个时期结束时评估验证损失和验证指标。\n",
    "\n",
    "还有一个选择：参数validation_split允许您自动保留部分训练数据以供验证。参数值代表要保留用于验证的数据的一部分，因此应将其设置为大于0且小于1的数字。例如，validation_split=0.2表示“使用20％的数据进行验证”，而validation_split=0.6表示“使用60％的数据用于验证”。\n",
    "\n",
    "验证的计算方法是在进行任何改组之前，对fit调用接收到的数组进行最后x％的采样。\n",
    "\n",
    "注意，只能validation_split在使用Numpy数据进行训练时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 2.2551 - accuracy: 0.1762 - val_loss: 2.2632 - val_accuracy: 0.1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba36f96c10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 模型验证\n",
    "\n",
    "返回 test loss 和metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3311 - accuracy: 0.0800\n",
      "test loss, test acc: [2.3310532569885254, 0.07999999821186066]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(x_test[:3])\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例2、使用样本加权和类别加权"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了输入数据和目标数据外，还可以在使用时将样本权重或类权重传递给模型fit：\n",
    "\n",
    "从Numpy数据进行训练时：通过sample_weight和class_weight参数。\n",
    "从数据集训练时：通过使数据集返回一个元组(input_batch, target_batch, sample_weight_batch)。\n",
    "“样本权重”数组是一个数字数组，用于指定批次中每个样本在计算总损失时应具有的权重。它通常用于不平衡的分类问题中（这种想法是为很少见的班级赋予更多的权重）。当所使用的权重为1和0时，该数组可用作损失函数的掩码（完全丢弃某些样本对总损失的贡献）。\n",
    "\n",
    "“类别权重”字典是同一概念的一个更具体的实例：它将类别索引映射到应该用于属于该类别的样本的样本权重。例如，如果在数据中类“ 0”的表示量比类“ 1”的表示量少两倍，则可以使用class_weight={0: 1., 1: 0.5}。\n",
    "\n",
    "这是一个Numpy示例，其中我们使用类权重或样本权重来更加**重视第5类的正确分类**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = tf.keras.Input(shape=(32,), name='digits')\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = tf.keras.layers.Dense(10, name='predictions')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit with class weight\n",
      "Epoch 1/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5293 - sparse_categorical_accuracy: 0.0940\n",
      "Epoch 2/4\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4942 - sparse_categorical_accuracy: 0.0960\n",
      "Epoch 3/4\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4833 - sparse_categorical_accuracy: 0.0960\n",
      "Epoch 4/4\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4747 - sparse_categorical_accuracy: 0.0960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba36e405d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#类别5：加权\n",
    "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
    "                5: 2.,\n",
    "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
    "\n",
    "\n",
    "print('Fit with class weight')\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          class_weight=class_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit with sample weight\n"
     ]
    }
   ],
   "source": [
    "# Here's the same example using `sample_weight` instead:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "\n",
    "sample_weight[y_train == 5] = 2.\n",
    "print('\\nFit with sample weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 2., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 1., 2., 1., 2., 2., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 2., 2., 1., 2., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 2., 1., 1., 1., 2., 2., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
       "       2., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       2., 2., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5146 - sparse_categorical_accuracy: 0.0960\n",
      "Epoch 2/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4862 - sparse_categorical_accuracy: 0.0990\n",
      "Epoch 3/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4758 - sparse_categorical_accuracy: 0.0980\n",
      "Epoch 4/4\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.4661 - sparse_categorical_accuracy: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba37e21e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          sample_weight=sample_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例3、使用回调函数\n",
    "\n",
    "Keras中的回调是在训练期间（在某个时期开始时，在批处理结束时，在某个时期结束时等）在不同时间点调用的对象，这些对象可用于实现以下行为：\n",
    "\n",
    "在训练过程中的不同时间点进行验证（除了内置的按时间段验证）\n",
    "\n",
    "定期或在超过特定精度阈值时对模型进行检查\n",
    "\n",
    "当训练似乎停滞不前时，更改模型的学习率\n",
    "\n",
    "当训练似乎停滞不前时，对顶层进行微调\n",
    "\n",
    "在训练结束或超出特定性能阈值时发送电子邮件或即时消息通知\n",
    "等等。\n",
    "回调可以作为列表传递给model.fit："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 EarlyStopping(早停)\n",
    "- monitor: 被监测的数据。\n",
    "- min_delta: 在被监测的数据中被认为是提升的最小变化， 例如，小于 min_delta 的绝对变化会被认为没有提升。\n",
    "- patience: 没有进步的训练轮数，在这之后训练就会被停止。\n",
    "- verbose: 详细信息模式。\n",
    "- mode: {auto, min, max} 其中之一。 在 min 模式中， 当被监测的数据停止下降，训练就会停止；在 max 模式中，当被监测的数据停止上升，训练就会停止；在 auto 模式中，方向会自动从被监测的数据的名字中判断出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored metric has stopped improving.\n",
      " |  \n",
      " |  Assuming the goal of a training is to minimize the loss. With this, the\n",
      " |  metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
      " |  `model.fit()` training loop will check at end of every epoch whether\n",
      " |  the loss is no longer decreasing, considering the `min_delta` and\n",
      " |  `patience` if applicable. Once it's found no longer decreasing,\n",
      " |  `model.stop_training` is marked True and the training terminates.\n",
      " |  \n",
      " |  The quantity to be monitored needs to be available in `logs` dict.\n",
      " |  To make it so, pass the loss or metrics at `model.compile()`.\n",
      " |  \n",
      " |  Arguments:\n",
      " |    monitor: Quantity to be monitored.\n",
      " |    min_delta: Minimum change in the monitored quantity\n",
      " |        to qualify as an improvement, i.e. an absolute\n",
      " |        change of less than min_delta, will count as no\n",
      " |        improvement.\n",
      " |    patience: Number of epochs with no improvement\n",
      " |        after which training will be stopped.\n",
      " |    verbose: verbosity mode.\n",
      " |    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |        training will stop when the quantity\n",
      " |        monitored has stopped decreasing; in `\"max\"`\n",
      " |        mode it will stop when the quantity\n",
      " |        monitored has stopped increasing; in `\"auto\"`\n",
      " |        mode, the direction is automatically inferred\n",
      " |        from the name of the monitored quantity.\n",
      " |    baseline: Baseline value for the monitored quantity.\n",
      " |        Training will stop if the model doesn't show improvement over the\n",
      " |        baseline.\n",
      " |    restore_best_weights: Whether to restore model weights from\n",
      " |        the epoch with the best value of the monitored quantity.\n",
      " |        If False, the model weights obtained at the last step of\n",
      " |        training are used.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
      " |  >>> # This callback will stop the training when there is no improvement in\n",
      " |  >>> # the validation loss for three consecutive epochs.\n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, batch_size=1, callbacks=[callback],\n",
      " |  ...                     verbose=0)\n",
      " |  >>> len(history.history['loss'])  # Only 4 epochs are run.\n",
      " |  4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.callbacks.EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 2.3245 - sparse_categorical_accuracy: 0.0913 - val_loss: 2.3089 - val_sparse_categorical_accuracy: 0.0850\n",
      "Epoch 2/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3015 - sparse_categorical_accuracy: 0.1163 - val_loss: 2.3039 - val_sparse_categorical_accuracy: 0.0750\n",
      "Epoch 3/20\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2929 - sparse_categorical_accuracy: 0.1213 - val_loss: 2.3076 - val_sparse_categorical_accuracy: 0.1050\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba380e6a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "\n",
    "#list\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # 当‘val_loss’不再下降时候停止训练 \n",
    "        monitor='val_loss',\n",
    "        # “不再下降”被定义为“减少不超过1e-2”\n",
    "        min_delta=1e-2,\n",
    "        # “不再改善”进一步定义为“至少2个epoch”\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 许多内置的回调可用\n",
    "- ModelCheckpoint：定期保存模型。\n",
    "- EarlyStopping：当培训不再改善验证指标时，停止培训。\n",
    "- TensorBoard：定期编写可在TensorBoard中可视化的模型日志（更多详细信息，请参见“可视化”部分）。\n",
    "- CSVLogger：将损失和指标数据流式传输到CSV文件。\n",
    "等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 checkpoint模型\n",
    "在相对较大的数据集上训练模型时，至关重要的是要定期保存模型的checkpoint。\n",
    "\n",
    "最简单的方法是使用ModelCheckpoint回调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelCheckpoint in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class ModelCheckpoint(Callback)\n",
      " |  ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch', options=None, **kwargs)\n",
      " |  \n",
      " |  Callback to save the Keras model or model weights at some frequency.\n",
      " |  \n",
      " |  `ModelCheckpoint` callback is used in conjunction with training using\n",
      " |  `model.fit()` to save a model or weights (in a checkpoint file) at some\n",
      " |  interval, so the model or weights can be loaded later to continue the training\n",
      " |  from the state saved.\n",
      " |  \n",
      " |  A few options this callback provides include:\n",
      " |  \n",
      " |  - Whether to only keep the model that has achieved the \"best performance\" so\n",
      " |    far, or whether to save the model at the end of every epoch regardless of\n",
      " |    performance.\n",
      " |  - Definition of 'best'; which quantity to monitor and whether it should be\n",
      " |    maximized or minimized.\n",
      " |  - The frequency it should save at. Currently, the callback supports saving at\n",
      " |    the end of every epoch, or after a fixed number of training batches.\n",
      " |  - Whether only weights are saved, or the whole model is saved.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  EPOCHS = 10\n",
      " |  checkpoint_filepath = '/tmp/checkpoint'\n",
      " |  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
      " |      filepath=checkpoint_filepath,\n",
      " |      save_weights_only=True,\n",
      " |      monitor='val_acc',\n",
      " |      mode='max',\n",
      " |      save_best_only=True)\n",
      " |  \n",
      " |  # Model weights are saved at the end of every epoch, if it's the best seen\n",
      " |  # so far.\n",
      " |  model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
      " |  \n",
      " |  # The model weights (that are considered the best) are loaded into the model.\n",
      " |  model.load_weights(checkpoint_filepath)\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |      filepath: string or `PathLike`, path to save the model file. `filepath`\n",
      " |        can contain named formatting options, which will be filled the value of\n",
      " |        `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if\n",
      " |        `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model\n",
      " |        checkpoints will be saved with the epoch number and the validation loss\n",
      " |        in the filename.\n",
      " |      monitor: quantity to monitor.\n",
      " |      verbose: verbosity mode, 0 or 1.\n",
      " |      save_best_only: if `save_best_only=True`, the latest best model according\n",
      " |        to the quantity monitored will not be overwritten.\n",
      " |        If `filepath` doesn't contain formatting options like `{epoch}` then\n",
      " |        `filepath` will be overwritten by each new better model.\n",
      " |      mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
      " |        overwrite the current save file is made based on either the maximization\n",
      " |        or the minimization of the monitored quantity. For `val_acc`, this\n",
      " |        should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
      " |        mode, the direction is automatically inferred from the name of the\n",
      " |        monitored quantity.\n",
      " |      save_weights_only: if True, then only the model's weights will be saved\n",
      " |        (`model.save_weights(filepath)`), else the full model is saved\n",
      " |        (`model.save(filepath)`).\n",
      " |      save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves\n",
      " |        the model after each epoch. When using integer, the callback saves the\n",
      " |        model at end of this many batches. If the `Model` is compiled with\n",
      " |        `experimental_steps_per_execution=N`, then the saving criteria will be\n",
      " |        checked every Nth batch. Note that if the saving isn't aligned to\n",
      " |        epochs, the monitored metric may potentially be less reliable (it\n",
      " |        could reflect as little as 1 batch, since the metrics get reset every\n",
      " |        epoch). Defaults to `'epoch'`.\n",
      " |      options: Optional `tf.train.CheckpointOptions` object if\n",
      " |        `save_weights_only` is true or optional `tf.saved_model.SavedOptions`\n",
      " |        object if `save_weights_only` is false.\n",
      " |      **kwargs: Additional arguments for backwards compatibility. Possible key\n",
      " |        is `period`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelCheckpoint\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch', options=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.callbacks.ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.3579 - sparse_categorical_accuracy: 0.1094\n",
      "Epoch 00001: val_loss improved from inf to 2.30772, saving model to mymodel_1\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 2.3241 - sparse_categorical_accuracy: 0.0913 - val_loss: 2.3077 - val_sparse_categorical_accuracy: 0.0950\n",
      "Epoch 2/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2819 - sparse_categorical_accuracy: 0.1719\n",
      "Epoch 00002: val_loss did not improve from 2.30772\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2963 - sparse_categorical_accuracy: 0.1125 - val_loss: 2.3109 - val_sparse_categorical_accuracy: 0.1150\n",
      "Epoch 3/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2873 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00003: val_loss did not improve from 2.30772\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2859 - sparse_categorical_accuracy: 0.1338 - val_loss: 2.3088 - val_sparse_categorical_accuracy: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba386aac10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}',\n",
    "        # 模型保存路径\n",
    "        #\n",
    "        # 下面的两个参数意味着当且仅当`val_loss`分数提高时，我们才会覆盖当前检查点。\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        #加入这个仅仅保存模型权重\n",
    "        save_weights_only=True,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3、使用回调实现动态学习率调整\n",
    "由于优化程序无法访问验证指标，因此无法使用这些计划对象来实现动态学习率计划（例如，当验证损失不再改善时降低学习率）。\n",
    "\n",
    "但是，回调确实可以访问所有指标，包括验证指标！因此，可以通过使用回调来修改优化程序上的当前学习率，从而实现此模式。实际上，它是作为ReduceLROnPlateau回调内置的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau参数\n",
    "\n",
    "- monitor: 被监测的指标。\n",
    "- factor: 学习速率被降低的因数。新的学习速率 = 学习速率 * 因数\n",
    "- patience: 没有进步的训练轮数，在这之后训练速率会被降低。\n",
    "- verbose: 整数。0：安静，1：更新信息。\n",
    "- mode: {auto, min, max} 其中之一。如果是 min 模式，学习速率会被降低如果被监测的数据已经停止下降； 在 max 模式，学习塑料会被降低如果被监测的数据已经停止上升； 在 auto 模式，方向会被从被监测的数据中自动推断出来。\n",
    "- min_delta: 衡量新的最佳阈值，仅关注重大变化。\n",
    "- cooldown: 在学习速率被降低之后，重新恢复正常操作之前等待的训练轮数量。\n",
    "- min_lr: 学习速率的下边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.3473 - sparse_categorical_accuracy: 0.1094\n",
      "Epoch 00001: val_loss improved from inf to 2.30631, saving model to mymodel_1\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 2.3248 - sparse_categorical_accuracy: 0.1125 - val_loss: 2.3063 - val_sparse_categorical_accuracy: 0.0950\n",
      "Epoch 2/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2852 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00002: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2986 - sparse_categorical_accuracy: 0.1238 - val_loss: 2.3065 - val_sparse_categorical_accuracy: 0.0850\n",
      "Epoch 3/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2913 - sparse_categorical_accuracy: 0.1094\n",
      "Epoch 00003: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2884 - sparse_categorical_accuracy: 0.1262 - val_loss: 2.3093 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 4/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2507 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00004: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2795 - sparse_categorical_accuracy: 0.1475 - val_loss: 2.3084 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 5/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2467 - sparse_categorical_accuracy: 0.2812\n",
      "Epoch 00005: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2722 - sparse_categorical_accuracy: 0.1437 - val_loss: 2.3145 - val_sparse_categorical_accuracy: 0.1050\n",
      "Epoch 6/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2499 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00006: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2640 - sparse_categorical_accuracy: 0.1525 - val_loss: 2.3150 - val_sparse_categorical_accuracy: 0.1150\n",
      "Epoch 7/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2436 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00007: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2571 - sparse_categorical_accuracy: 0.1663 - val_loss: 2.3106 - val_sparse_categorical_accuracy: 0.1250\n",
      "Epoch 8/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2434 - sparse_categorical_accuracy: 0.1719\n",
      "Epoch 00008: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2494 - sparse_categorical_accuracy: 0.1838 - val_loss: 2.3137 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 9/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2329 - sparse_categorical_accuracy: 0.2031\n",
      "Epoch 00009: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2417 - sparse_categorical_accuracy: 0.1813 - val_loss: 2.3137 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 10/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2515 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00010: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2344 - sparse_categorical_accuracy: 0.1825 - val_loss: 2.3195 - val_sparse_categorical_accuracy: 0.1250\n",
      "Epoch 11/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2211 - sparse_categorical_accuracy: 0.2969\n",
      "Epoch 00011: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2235 - sparse_categorical_accuracy: 0.2075 - val_loss: 2.3174 - val_sparse_categorical_accuracy: 0.1200\n",
      "Epoch 12/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1951 - sparse_categorical_accuracy: 0.1719\n",
      "Epoch 00012: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2187 - sparse_categorical_accuracy: 0.2037 - val_loss: 2.3199 - val_sparse_categorical_accuracy: 0.1100\n",
      "Epoch 13/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2307 - sparse_categorical_accuracy: 0.2344\n",
      "Epoch 00013: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2148 - sparse_categorical_accuracy: 0.2075 - val_loss: 2.3194 - val_sparse_categorical_accuracy: 0.0950\n",
      "Epoch 14/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2179 - sparse_categorical_accuracy: 0.2656\n",
      "Epoch 00014: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2087 - sparse_categorical_accuracy: 0.2150 - val_loss: 2.3190 - val_sparse_categorical_accuracy: 0.1050\n",
      "Epoch 15/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2064 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00015: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2057 - sparse_categorical_accuracy: 0.2150 - val_loss: 2.3188 - val_sparse_categorical_accuracy: 0.1100\n",
      "Epoch 16/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2043 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00016: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2036 - sparse_categorical_accuracy: 0.2175 - val_loss: 2.3191 - val_sparse_categorical_accuracy: 0.1050\n",
      "Epoch 17/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2329 - sparse_categorical_accuracy: 0.2031\n",
      "Epoch 00017: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2007 - sparse_categorical_accuracy: 0.2212 - val_loss: 2.3193 - val_sparse_categorical_accuracy: 0.1100\n",
      "Epoch 18/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1930 - sparse_categorical_accuracy: 0.2500\n",
      "Epoch 00018: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1996 - sparse_categorical_accuracy: 0.2188 - val_loss: 2.3198 - val_sparse_categorical_accuracy: 0.1050\n",
      "Epoch 19/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1729 - sparse_categorical_accuracy: 0.2188\n",
      "Epoch 00019: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1986 - sparse_categorical_accuracy: 0.2188 - val_loss: 2.3201 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 20/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2049 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00020: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1969 - sparse_categorical_accuracy: 0.2188 - val_loss: 2.3203 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 21/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1730 - sparse_categorical_accuracy: 0.2656\n",
      "Epoch 00021: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1963 - sparse_categorical_accuracy: 0.2237 - val_loss: 2.3206 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 22/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1813 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00022: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1957 - sparse_categorical_accuracy: 0.2200 - val_loss: 2.3207 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 23/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1755 - sparse_categorical_accuracy: 0.2031\n",
      "Epoch 00023: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1949 - sparse_categorical_accuracy: 0.2150 - val_loss: 2.3207 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 24/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1482 - sparse_categorical_accuracy: 0.2812\n",
      "Epoch 00024: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1947 - sparse_categorical_accuracy: 0.2175 - val_loss: 2.3206 - val_sparse_categorical_accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1704 - sparse_categorical_accuracy: 0.2500\n",
      "Epoch 00025: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1945 - sparse_categorical_accuracy: 0.2225 - val_loss: 2.3207 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 26/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1963 - sparse_categorical_accuracy: 0.2188\n",
      "Epoch 00026: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1940 - sparse_categorical_accuracy: 0.2200 - val_loss: 2.3207 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 27/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2059 - sparse_categorical_accuracy: 0.1719\n",
      "Epoch 00027: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1939 - sparse_categorical_accuracy: 0.2175 - val_loss: 2.3207 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 28/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2147 - sparse_categorical_accuracy: 0.2500\n",
      "Epoch 00028: val_loss did not improve from 2.30631\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1937 - sparse_categorical_accuracy: 0.2200 - val_loss: 2.3208 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 29/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2182 - sparse_categorical_accuracy: 0.2500\n",
      "Epoch 00029: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.1935 - sparse_categorical_accuracy: 0.2188 - val_loss: 2.3208 - val_sparse_categorical_accuracy: 0.1000\n",
      "Epoch 30/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.1789 - sparse_categorical_accuracy: 0.2812\n",
      "Epoch 00030: val_loss did not improve from 2.30631\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.1934 - sparse_categorical_accuracy: 0.2175 - val_loss: 2.3208 - val_sparse_categorical_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba19473dd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}',\n",
    "        # 模型保存路径\n",
    "        # 下面的两个参数意味着当且仅当`val_loss`分数提高时，我们才会覆盖当前检查点。\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        #加入这个仅仅保存模型权重\n",
    "        save_weights_only=True,\n",
    "        verbose=1),\n",
    "    \n",
    "    \n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_sparse_categorical_accuracy\", \n",
    "                                         verbose=1, \n",
    "                                         mode='max', \n",
    "                                         factor=0.5, \n",
    "                                         patience=3)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=30,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例4、将数据传递到多输入，多输出模型\n",
    "在前面的示例中，我们正在考虑一个具有单个输入（shape的张量(32,)）和单个输出（shape的预测张量(10,)）的模型。但是具有多个输入或输出的模型呢？\n",
    "\n",
    "考虑以下模型，该模型具有形状的图像输入(32, 32, 3)（即(height, width, channels)）和形状的时间序列输入(None, 10)（即(timesteps, features)）。我们的模型将具有根据这些输入的组合计算出的两个输出：“得分”（形状(1,)）和五类（形状(5,)）的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_input = tf.keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = tf.keras.Input(shape=(20, 10), name='ts_input')\n",
    "\n",
    "x1 = tf.keras.layers.Conv2D(3, 3)(image_input)\n",
    "x1 = tf.keras.layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = tf.keras.layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = tf.keras.layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = tf.keras.layers.Dense(1, name='score_output')(x)\n",
    "class_output = tf.keras.layers.Dense(5, name='class_output')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们绘制这个模型，以便您可以清楚地看到我们在这里做什么（请注意，图中显示的形状是批处理形状，而不是按样本的形状）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot_model in module tensorflow.python.keras.utils.vis_utils:\n",
      "\n",
      "plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)\n",
      "    Converts a Keras model to dot format and save to a file.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    input = tf.keras.Input(shape=(100,), dtype='int32', name='input')\n",
      "    x = tf.keras.layers.Embedding(\n",
      "        output_dim=512, input_dim=10000, input_length=100)(input)\n",
      "    x = tf.keras.layers.LSTM(32)(x)\n",
      "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
      "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
      "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
      "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
      "    model = tf.keras.Model(inputs=[input], outputs=[output])\n",
      "    dot_img_file = '/tmp/model_1.png'\n",
      "    tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
      "    ```\n",
      "    \n",
      "    Arguments:\n",
      "      model: A Keras model instance\n",
      "      to_file: File name of the plot image.\n",
      "      show_shapes: whether to display shape information.\n",
      "      show_layer_names: whether to display layer names.\n",
      "      rankdir: `rankdir` argument passed to PyDot,\n",
      "          a string specifying the format of the plot:\n",
      "          'TB' creates a vertical plot;\n",
      "          'LR' creates a horizontal plot.\n",
      "      expand_nested: Whether to expand nested models into clusters.\n",
      "      dpi: Dots per inch.\n",
      "    \n",
      "    Returns:\n",
      "      A Jupyter notebook Image object if Jupyter is installed.\n",
      "      This enables in-line display of the model plots in notebooks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.utils.plot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "#https://blog.csdn.net/weixin_42459037/article/details/84066164\n",
    "\n",
    "tf.keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True,dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 损失函数\n",
    "在编译时，通过将损失函数作为列表传递，我们可以为不同的输出指定不同的损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们仅将单个损失函数传递给模型，则将相同的损失函数应用于每个输出，这在此处不合适。\n",
    "\n",
    "### 4.2指标函数\n",
    "同样对于指标：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)],\n",
    "    metrics=[\n",
    "        [tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "              tf.keras.metrics.MeanAbsoluteError()],\n",
    "        \n",
    "             [tf.keras.metrics.CategoricalAccuracy()]\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们为输出层命名，因此我们还可以通过dict指定每个输出的损失和指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': tf.keras.losses.MeanSquaredError(),\n",
    "          'class_output': tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "         },\n",
    "    metrics={'score_output': [tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              tf.keras.metrics.MeanAbsoluteError()],\n",
    "             \n",
    "             'class_output': [tf.keras.metrics.CategoricalAccuracy()]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您有两个以上的输出，我们建议使用显式名称和字典。\n",
    "\n",
    "可以使用以下参数对不同的特定于输出的损失赋予不同的权重（例如，在我们的示例中，我们可能希望通过将某类损失函数赋予更高的权重）\n",
    "\n",
    "loss_weights："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': tf.keras.losses.MeanSquaredError(),\n",
    "          'class_output': tf.keras.losses.CategoricalCrossentropy(from_logits=True)},\n",
    "    metrics={'score_output': [tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              tf.keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [tf.keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weights={'score_output': 2., 'class_output': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您还可以选择不为某些输出计算损失，如果这些输出仅用于预测而不是训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, tf.keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'class_output':tf.keras.losses.CategoricalCrossentropy(from_logits=True)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3完整运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_input = tf.keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = tf.keras.Input(shape=(20, 10), name='ts_input')\n",
    "\n",
    "x1 = tf.keras.layers.Conv2D(3, 3)(image_input)\n",
    "x1 = tf.keras.layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = tf.keras.layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = tf.keras.layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = tf.keras.layers.Dense(1, name='score_output')(x)\n",
    "class_output = tf.keras.layers.Dense(5, name='class_output')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 3s 26ms/sample - loss: 4.6500 - score_output_loss: 0.0963 - class_output_loss: 4.6443\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 400us/sample - loss: 4.5148 - score_output_loss: 0.0766 - class_output_loss: 4.6994\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 900us/sample - loss: 4.4384 - score_output_loss: 0.0813 - class_output_loss: 4.6595\n",
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 0s 800us/sample - loss: 4.4204 - score_output_loss: 0.0807 - class_output_loss: 4.2080\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 800us/sample - loss: 4.4159 - score_output_loss: 0.0945 - class_output_loss: 4.3787\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 300us/sample - loss: 4.4291 - score_output_loss: 0.1037 - class_output_loss: 4.3664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29c0cc6c080>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "import numpy as np\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 11ms/sample - loss: 4.8652 - score_output_loss: 0.1548 - class_output_loss: 4.8238\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 300us/sample - loss: 4.9714 - score_output_loss: 0.1119 - class_output_loss: 5.0570\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 545us/sample - loss: 5.0455 - score_output_loss: 0.1024 - class_output_loss: 4.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29c09d71ba8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "import numpy as np\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit((img_data,  ts_data),\n",
    "          (score_targets, class_targets),\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
