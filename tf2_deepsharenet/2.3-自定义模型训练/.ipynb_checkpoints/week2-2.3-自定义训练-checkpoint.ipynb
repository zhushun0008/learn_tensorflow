{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GradientTape**是eager模式下计算梯度用的\n",
    "\n",
    "- **watch(tensor)**\n",
    "\n",
    "  作用：确保某个tensor被tape追踪 \n",
    "\n",
    "  参数:tensor: 一个Tensor或者一个Tensor列表\n",
    "\n",
    "- **gradient(target, sources)**\n",
    "\n",
    "  作用：根据tape上面的上下文来计算某个或者某些tensor的梯度参数\n",
    "\n",
    "  target: 被微分的Tensor或者Tensor列表，你可以理解为经过某个函数之后的值\n",
    "\n",
    "  sources: Tensors 或者Variables列表（当然可以只有一个值）. 你可以理解为函数的某个变量\n",
    "\n",
    "  返回:\n",
    "  \n",
    "  一个列表表示各个变量的梯度值，和source中的变量列表一一对应，表明这个变量的梯度。\n",
    "  \n",
    "  上面的例子中的梯度计算部分可以更直观的理解这个函数的用法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x)  # y’ = 2*x = 2*3 = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例1、模型自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型（神经网络的前向传播） --> 定义损失函数  --> 定义优化函数 --> 定义tape  -->  模型得到预测值  --> 前向传播得到loss --> 反向传播 --> 用优化函数将计算出来的梯度更新到变量上面去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu') #隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)#输出层\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 10分类问题\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(data)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    \n",
    "gradients = tape.gradient(loss, model.trainable_variables) #求梯度\n",
    "\n",
    "optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_model/dense/kernel:0' shape=(32, 32) dtype=float32, numpy=\n",
       " array([[ 0.24546336, -0.2460907 ,  0.02016227, ..., -0.05905663,\n",
       "          0.2612972 ,  0.24162032],\n",
       "        [-0.17313091,  0.13822436, -0.14858948, ..., -0.0343163 ,\n",
       "          0.24080767,  0.04156224],\n",
       "        [ 0.27418938,  0.20402321, -0.17353994, ..., -0.11113551,\n",
       "         -0.23375006,  0.29341274],\n",
       "        ...,\n",
       "        [-0.14500201,  0.21758285, -0.12902828, ..., -0.24561772,\n",
       "         -0.23459432,  0.12809373],\n",
       "        [-0.06625047, -0.26783645, -0.11974709, ..., -0.18917704,\n",
       "          0.06630273, -0.27170613],\n",
       "        [ 0.09076995, -0.17777508,  0.14847633, ..., -0.2831062 ,\n",
       "         -0.19575565,  0.20660914]], dtype=float32)>,\n",
       " <tf.Variable 'my_model/dense/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([ 0.00099996,  0.        ,  0.00099397, -0.00099994, -0.00099987,\n",
       "        -0.00099995,  0.00099953,  0.00099913,  0.00099999, -0.00099949,\n",
       "        -0.00099898,  0.00099956, -0.00099998, -0.0009999 ,  0.00099997,\n",
       "         0.00099824,  0.00099992,  0.00099999, -0.00099997, -0.00099785,\n",
       "         0.00099994,  0.00099842,  0.00099996,  0.00099999, -0.0009999 ,\n",
       "        -0.00099997,  0.00099998,  0.00099082, -0.00099985,  0.00099962,\n",
       "        -0.00099773,  0.00099998], dtype=float32)>,\n",
       " <tf.Variable 'my_model/dense_1/kernel:0' shape=(32, 10) dtype=float32, numpy=\n",
       " array([[ 1.62177935e-01,  3.36939126e-01,  7.86084384e-02,\n",
       "          2.16904387e-01, -8.67203474e-02, -2.10530654e-01,\n",
       "          2.21637592e-01,  2.98658460e-01, -2.32326239e-02,\n",
       "          2.42882162e-01],\n",
       "        [-1.64271876e-01,  1.93473667e-01,  2.44253248e-01,\n",
       "         -2.75214523e-01, -2.21874923e-01,  7.97391236e-02,\n",
       "          1.32874638e-01,  1.79437667e-01,  1.43709451e-01,\n",
       "         -1.25246674e-01],\n",
       "        [ 1.70420036e-01,  1.14925131e-01, -1.09982535e-01,\n",
       "          3.40600967e-01, -2.31666565e-01, -3.20295513e-01,\n",
       "         -3.00558954e-01,  3.46068323e-01, -9.33339670e-02,\n",
       "          1.71309292e-01],\n",
       "        [ 3.40982616e-01,  1.71681210e-01, -1.80021059e-02,\n",
       "         -6.95494339e-02, -1.71273336e-01, -1.47793675e-02,\n",
       "         -3.22610170e-01,  6.69684038e-02, -2.99440324e-01,\n",
       "          1.74419418e-01],\n",
       "        [ 3.76893312e-01,  1.18680000e-01, -2.03413472e-01,\n",
       "         -1.90553159e-01, -3.33418883e-02,  5.82767725e-02,\n",
       "         -1.08009599e-01,  1.12443961e-01, -7.60112628e-02,\n",
       "         -1.10988587e-01],\n",
       "        [-3.42205256e-01, -8.64807889e-02,  1.19482763e-01,\n",
       "         -2.82874972e-01, -2.79503524e-01, -4.39822935e-02,\n",
       "         -1.24502711e-01, -1.85919017e-01,  2.16461405e-01,\n",
       "          1.10723399e-01],\n",
       "        [ 1.86062921e-02, -2.61676669e-01, -3.22655886e-01,\n",
       "          2.75651544e-01, -3.16726029e-01,  9.74376202e-02,\n",
       "          1.97409630e-01,  3.11698645e-01,  1.79006815e-01,\n",
       "         -1.59882694e-01],\n",
       "        [ 1.90501824e-01,  2.96757281e-01, -1.97531402e-01,\n",
       "          3.40622336e-01, -3.04841459e-01,  2.55878359e-01,\n",
       "          8.40005875e-02,  7.19959065e-02,  3.16019386e-01,\n",
       "         -2.66895443e-01],\n",
       "        [ 1.51953846e-01,  3.69029075e-01,  1.46528989e-01,\n",
       "          2.82895919e-02, -8.96693990e-02,  4.26237956e-02,\n",
       "          1.52147859e-01,  1.55022711e-01, -1.42750010e-01,\n",
       "          9.84535888e-02],\n",
       "        [-2.73188472e-01,  2.87110716e-01, -2.04226926e-01,\n",
       "         -1.22214958e-01,  3.73914093e-01, -4.58691418e-02,\n",
       "         -2.77860999e-01, -1.16526952e-03,  1.92121178e-01,\n",
       "         -3.51274401e-01],\n",
       "        [ 8.14675763e-02, -3.07377517e-01,  4.24127243e-02,\n",
       "          9.83834341e-02, -2.77749151e-01, -4.42772880e-02,\n",
       "         -2.04204395e-01, -3.46727848e-01,  3.53634149e-01,\n",
       "         -8.48837383e-03],\n",
       "        [ 3.11042994e-01, -6.97910190e-02,  6.52042404e-02,\n",
       "          2.96228528e-02,  1.37888223e-01,  3.04276615e-01,\n",
       "         -2.11110622e-01,  5.33518270e-02,  1.19296737e-01,\n",
       "          3.12479526e-01],\n",
       "        [ 6.07785434e-02,  2.82990113e-02, -7.26660341e-02,\n",
       "         -1.83133036e-02,  1.01526722e-01, -1.95474952e-01,\n",
       "         -3.38435799e-01, -6.02249317e-02,  3.56352814e-02,\n",
       "          3.89801688e-04],\n",
       "        [-3.31856042e-01,  5.30873705e-03,  2.55235255e-01,\n",
       "         -2.08115786e-01,  1.47015199e-01, -3.58443201e-01,\n",
       "         -2.29932860e-01, -5.28912358e-02,  2.13303879e-01,\n",
       "          3.10638160e-01],\n",
       "        [-1.89451650e-01, -3.87197360e-02,  3.09401024e-02,\n",
       "          2.44082466e-01,  3.77696097e-01,  1.49451926e-01,\n",
       "         -1.47558406e-01, -2.93158680e-01,  2.30477765e-01,\n",
       "         -6.37765136e-03],\n",
       "        [ 1.98372453e-01, -2.94767678e-01, -2.62599625e-02,\n",
       "         -5.52679896e-02,  5.82160316e-02,  2.93119311e-01,\n",
       "         -3.64551306e-01, -2.72213072e-01,  3.13909799e-01,\n",
       "          3.66376936e-01],\n",
       "        [ 1.42404944e-01,  2.66581684e-01,  2.24335529e-02,\n",
       "          3.11319321e-01,  3.60523500e-02, -1.32991418e-01,\n",
       "         -2.50236511e-01,  1.48739249e-01, -3.31760883e-01,\n",
       "          7.04640746e-02],\n",
       "        [ 3.70694876e-01, -5.43969423e-02, -1.53295234e-01,\n",
       "          1.85586989e-01,  3.43890011e-01, -1.38786748e-01,\n",
       "         -2.66798645e-01,  3.09662819e-01,  1.31931007e-01,\n",
       "          1.16263568e-01],\n",
       "        [ 2.19105706e-01,  1.71236426e-01,  1.07983910e-02,\n",
       "          3.35762203e-01, -1.53298318e-01, -3.71463686e-01,\n",
       "          6.93321181e-03, -7.61473458e-03, -2.25196004e-01,\n",
       "         -3.17101210e-01],\n",
       "        [-3.02221209e-01, -1.17434449e-01,  3.16895157e-01,\n",
       "          1.30099118e-01, -3.39023203e-01,  9.23857167e-02,\n",
       "         -1.37513846e-01, -3.19574773e-01,  1.22526266e-01,\n",
       "         -1.04605168e-01],\n",
       "        [ 3.23785454e-01,  2.05600828e-01,  1.21189214e-01,\n",
       "          1.31550103e-01, -9.81956944e-02,  2.42735356e-01,\n",
       "         -3.33220303e-01, -3.06235105e-01, -2.94405341e-01,\n",
       "          3.04914564e-01],\n",
       "        [-3.56082708e-01,  7.25418180e-02,  2.88889796e-01,\n",
       "         -1.97171703e-01, -3.84165272e-02,  2.83957422e-01,\n",
       "          2.25324882e-04, -1.51853621e-01,  3.66783828e-01,\n",
       "         -1.91199392e-01],\n",
       "        [-2.64554530e-01, -3.69676411e-01,  2.58418620e-01,\n",
       "          3.60976547e-01,  3.65815461e-01, -2.41101421e-02,\n",
       "          1.70637727e-01, -2.99927175e-01, -1.53862327e-01,\n",
       "          3.00324500e-01],\n",
       "        [-1.92802787e-01,  6.25318289e-02,  2.40531713e-01,\n",
       "         -2.29730859e-01,  3.43884975e-01,  1.00193150e-01,\n",
       "          3.89469936e-02,  2.09355861e-01,  2.03636345e-02,\n",
       "         -5.25057577e-02],\n",
       "        [ 1.02510154e-01, -5.71130253e-02,  1.92615420e-01,\n",
       "         -4.76292670e-02,  2.95649678e-01,  1.16550326e-01,\n",
       "         -3.55631858e-01,  1.79414839e-01, -1.75299451e-01,\n",
       "         -2.72246003e-01],\n",
       "        [-2.93943316e-01, -1.85080141e-01, -2.47901872e-01,\n",
       "         -2.57406503e-01,  2.75679320e-01, -1.46245718e-01,\n",
       "         -3.92039195e-02,  1.99563161e-01,  2.19809979e-01,\n",
       "          6.59557804e-02],\n",
       "        [ 2.02370137e-01,  3.33503157e-01, -1.74082741e-01,\n",
       "          3.48324448e-01, -1.03815645e-01, -1.21500731e-01,\n",
       "          6.14280142e-02, -1.07638270e-01,  1.87446088e-01,\n",
       "         -1.73725531e-01],\n",
       "        [ 2.46514618e-01,  3.35765570e-01,  1.88810227e-03,\n",
       "         -1.80103362e-01,  8.94793402e-03,  1.86048999e-01,\n",
       "         -1.27073720e-01,  1.98684648e-01, -3.21683794e-01,\n",
       "          1.21366359e-01],\n",
       "        [ 2.08735108e-01,  2.14595973e-01,  7.04305014e-03,\n",
       "          1.39959052e-01,  3.11135739e-01, -3.02118868e-01,\n",
       "         -1.25847340e-01, -3.41029257e-01, -3.33372802e-01,\n",
       "          2.55292177e-01],\n",
       "        [ 3.71744514e-01, -2.61226118e-01, -2.68147558e-01,\n",
       "          3.01155448e-01, -2.45589942e-01,  1.59475267e-01,\n",
       "          1.35580719e-01,  1.61254033e-03, -3.48775119e-01,\n",
       "          2.00162888e-01],\n",
       "        [ 3.60439032e-01, -1.42200261e-01, -1.30138263e-01,\n",
       "         -3.16615887e-02, -1.09631464e-01, -1.08654849e-01,\n",
       "         -1.10297084e-01, -3.73272419e-01, -3.35612744e-01,\n",
       "          1.91486686e-01],\n",
       "        [-3.42547327e-01,  1.76982775e-01, -2.12456167e-01,\n",
       "          2.39220336e-01,  4.71852571e-02,  2.03210965e-01,\n",
       "          3.01923066e-01, -7.64645115e-02, -1.45215783e-02,\n",
       "          3.00820261e-01]], dtype=float32)>,\n",
       " <tf.Variable 'my_model/dense_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.00099999, 0.00099999, 0.00099999, 0.00099999, 0.00099999,\n",
       "        0.00099999, 0.00099999, 0.00099999, 0.00099999, 0.00099999],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例2：使用GradientTape自定义训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "#batch_size\n",
    "#tape 求梯度  梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 38.283538818359375\n",
      "Seen so far: 64 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 37.04734802246094\n",
      "Seen so far: 64 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 32.42188262939453\n",
      "Seen so far: 64 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "\n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        \n",
    "        # 打开GradientTape以记录正向传递期间运行的操作，这将启用自动区分。\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # 运行该模型的前向传播。 模型应用于其输入的操作将记录在GradientTape上。\n",
    "            logits = model(x_batch_train, training=True)  # 这个minibatch的预测值\n",
    "\n",
    "            # 计算这个minibatch的损失值\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # 使用GradientTape自动获取可训练变量相对于损失的梯度。\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # 通过更新变量的值来最大程度地减少损失，从而执行梯度下降的一步。\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 每200 batches打印一次.\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例3：使用GradientTape自定义训练模型进阶（加入评估函数）\n",
    "让我们将metric添加到组合中。下面可以在从头开始编写的训练循环中随时使用内置指标（或编写的自定义指标）。流程如下：\n",
    "\n",
    "- 在循环开始时初始化metrics\n",
    "- metric.update_state():每batch之后更新\n",
    "- metric.result():需要显示metrics的当前值时调用\n",
    "- metric.reset_states():需要清除metrics状态时重置（通常在每个epoch的结尾）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 32))\n",
    "y_train = np.random.random((1000, 10))\n",
    "x_val = np.random.random((200, 32))\n",
    "y_val = np.random.random((200, 10))\n",
    "x_test = np.random.random((200, 32))\n",
    "y_test = np.random.random((200, 10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# 损失函数\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 准备metrics函数\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# 准备训练数据集\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 准备测试数据集\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行几个epoch运行训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training acc over epoch: 0.10700000077486038\n",
      "Validation acc: 0.0949999988079071\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.10400000214576721\n",
      "Validation acc: 0.0949999988079071\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.10300000011920929\n",
      "Validation acc: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        \n",
    "        #一个batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))####\n",
    "\n",
    "        # 更新训练集的metrics\n",
    "        train_acc_metric(y_batch_train, logits)     \n",
    "            \n",
    "            \n",
    "    # 在每个epoch结束时显示metrics。\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    # 在每个epoch结束时重置训练指标\n",
    "    train_acc_metric.reset_states()#!!!!!!!!!!!!!!!\n",
    "\n",
    "    # 在每个epoch结束时运行一个验证集。\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        # 更新验证集merics\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    print('Validation acc: %s' % (float(val_acc),))\n",
    "    val_acc_metric.reset_states()\n",
    "    \n",
    "    #显示测试集\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
