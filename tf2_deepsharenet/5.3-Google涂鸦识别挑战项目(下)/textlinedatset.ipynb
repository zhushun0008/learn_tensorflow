{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "\n",
    "DP_DIR = './shuffle_data_gzip/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_SIZE = 256\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "np.random.seed(seed=1987)\n",
    "\n",
    "\n",
    "\n",
    "STEPS = 800\n",
    "EPOCHS = 16\n",
    "size = 64\n",
    "batchsize = 680\n",
    "NCATS =340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = glob.glob(\"./shuffle_data_gzip/*.csv.gz\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./shuffle_data_gzip/train_k52.csv.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('./shuffle_data_gzip/train_k52.csv.gz',sep=',').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(line):\n",
    "    column_default = [tf.constant(\"0\",dtype=tf.string), #drawing\n",
    "                      tf.constant(0,dtype=tf.int32)] #label\n",
    "    columns = tf.io.decode_csv(line, column_default, select_cols=[1,5])\n",
    "    features = columns[0]\n",
    "    label = columns[1]\n",
    "    return features, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size=64, lw=6):\n",
    "    raw_strokes = eval(raw_strokes.numpy())\n",
    "    img = np.zeros((256, 256), np.uint8)\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, lw)\n",
    "    return cv2.resize(img, (size, size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_draw_cv2(image, label):\n",
    "    [image] = tf.py_function(draw_cv2, [image], [tf.float32])\n",
    "    image = tf.reshape(image,(64,64,1))\n",
    "    label = tf.one_hot(label,depth=NCATS)\n",
    "    image.set_shape((64,64,1))\n",
    "    label.set_shape((340,))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(fileList[:-1])\n",
    "train_ds = train_ds.interleave(lambda x:\n",
    "    tf.data.TextLineDataset(x,compression_type='GZIP').skip(1).map(parse_csv,num_parallel_calls=tf.data.experimental.AUTOTUNE),\n",
    "    cycle_length=4, block_length=16,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#drawing处理\n",
    "train_ds = train_ds.map(tf_draw_cv2,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# tensor \n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).shuffle(3000).batch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.TextLineDataset(fileList[5],compression_type='GZIP').skip(1).map(parse_csv)  \n",
    "# train_ds = train_ds.map(tf_draw_cv2)\n",
    "# train_ds = train_ds.shuffle(3000).batch(1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [10:24,  1.77s/it]"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import tqdm\n",
    "# for file in fileList:\n",
    "#     try:\n",
    "#         train_ds = tf.data.TextLineDataset(file,compression_type='GZIP').skip(1).map(parse_csv)  \n",
    "#         train_ds = train_ds.map(tf_draw_cv2,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#         train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE).shuffle(3000).batch(10240)\n",
    "\n",
    "#         for image,label in tqdm.tqdm(train_ds):\n",
    "#             time.sleep(0.0000000000001)\n",
    "#     except:\n",
    "#         print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_csv(line):\n",
    "#     column_default = [tf.constant(\"0\",dtype=tf.string),\n",
    "#                       tf.constant(\"0\",dtype=tf.string),\n",
    "#                       tf.constant(\"0\",dtype=tf.string),\n",
    "#                       tf.constant(\"0\",dtype=tf.string),\n",
    "#                       tf.constant(\"0\",dtype=tf.string),\n",
    "#                       tf.constant(0,dtype=tf.int32),\n",
    "#                       tf.constant(0,dtype=tf.int32)]\n",
    "#     columns = tf.io.decode_csv(line, column_default)\n",
    "#     label = columns[-2] \n",
    "#     features = columns[1]\n",
    "#     return features, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_cv2(raw_strokes, size=64, lw=6):\n",
    "#     raw_strokes = eval(raw_strokes.numpy())\n",
    "#     img = np.zeros((256, 256), np.uint8)\n",
    "#     for stroke in raw_strokes:\n",
    "#         for i in range(len(stroke[0]) - 1):\n",
    "#             _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), 255, lw)\n",
    "#     return cv2.resize(img, (size, size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_draw_cv2(image, label):\n",
    "#     [image] = tf.py_function(draw_cv2, [image], [tf.float32])\n",
    "#     image = tf.reshape(image,(64,64,1))\n",
    "#     label = tf.one_hot(label,depth=NCATS)\n",
    "#     image.set_shape((64,64,1))\n",
    "#     label.set_shape((340,))\n",
    "#     return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.TextLineDataset(fileList[5],compression_type='GZIP').skip(1).map(parse_csv)  \n",
    "# train_ds = train_ds.map(tf_draw_cv2)\n",
    "# train_ds = train_ds.shuffle(3000).batch(10240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import tqdm\n",
    "# for file in fileList:\n",
    "#     try:\n",
    "#         train_ds = tf.data.TextLineDataset(file,compression_type='GZIP').skip(1).map(parse_csv)  \n",
    "#         train_ds = train_ds.map(tf_draw_cv2)\n",
    "#         train_ds = train_ds.shuffle(3000).batch(10240)\n",
    "\n",
    "#         for image,label in tqdm.tqdm(train_ds):\n",
    "#             time.sleep(0.0000000000001)\n",
    "#     except:\n",
    "#         print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet论文参考： https://arxiv.org/pdf/1704.04861.pdf\n",
    "\n",
    "tf.keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\n",
    "- input_shape: 可选，输入尺寸元组，仅当 include_top=False 时有效，否则输入形状必须是 (224, 224, 3)（channels_last 格式）或 (3, 224, 224)（channels_first 格式）。它必须为 3 个输入通道，且宽高必须不小于 32，比如 (200, 200, 3) 是一个合法的输入尺寸。\n",
    "- alpha: 控制网络的宽度：\n",
    "- 如果 alpha < 1.0，则同比例减少每层的滤波器个数。\n",
    "- 如果 alpha > 1.0，则同比例增加每层的滤波器个数。\n",
    "- 如果 alpha = 1，使用论文默认的滤波器个数\n",
    "- depth_multiplier: depthwise卷积的深度乘子，也称为（分辨率乘子）\n",
    "- dropout: dropout 概率\n",
    "- include_top: 是否包括顶层的全连接层。\n",
    "- weights: None 代表随机初始化， 'imagenet' 代表加载在 ImageNet 上预训练的权值。\n",
    "- input_tensor: 可选，Keras tensor 作为模型的输入（比如 layers.Input() 输出的 tensor）。\n",
    "- pooling: 可选，当 include_top 为 False 时，该参数指定了特征提取时的池化方式。 None 代表不池化，直接输出最后一层卷积层的输出，该输出是一个四维张量。'avg' 代表全局平均池化（GlobalAveragePooling2D），相当于在最后一层卷积层后面再加一层全局平均池化层，输出是一个二维张量。 'max' 代表全局最大池化\n",
    "- classes: 可选，图片分类的类别数，仅当 include_top 为 True 并且不加载预训练权值时可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MobileNetModel(tf.keras.models.Model):\n",
    "    def __init__(self, size, n_labels, **kwargs):\n",
    "        super(MobileNetModel, self).__init__(**kwargs)\n",
    "        self.base_model = tf.keras.applications.MobileNet(input_shape=(size, size, 1), include_top=False, weights=None, classes=n_labels)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.outputs =  tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        output_ = self.outputs(x)\n",
    "        return output_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetModel(size=64,n_labels=NCATS)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "train_top3_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=3,name='train_top_3_categorical_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_top3_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=3,name='test_top_3_categorical_accuracy')\n",
    "\n",
    "# @tf.function\n",
    "#setp\n",
    "def train_one_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "     \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    train_top3_accuracy(labels, predictions)\n",
    "#step\n",
    "def val_one_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    test_top3_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 64, 64, 1) (1024, 340)\n"
     ]
    }
   ],
   "source": [
    "for a,b in train_ds.take(1):\n",
    "    print(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.TextLineDataset(fileList[-1],compression_type='GZIP').skip(1).map(parse_csv,num_parallel_calls=tf.data.experimental.AUTOTUNE)  \n",
    "val_ds = val_ds.map(tf_draw_cv2,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.batch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "for epoch in range(EPOCHS):\n",
    "    # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_top3_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    test_top3_accuracy.reset_states()\n",
    "\n",
    "    for step,(images, labels) in enumerate(train_ds):\n",
    "        train_one_step(images, labels)\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\"step:{0}; Samples:{1}; Train Loss:{2}; Train Accuracy:{3},Train Top3 Accuracy:{4}\".format(step, (step + 1) * 1024, \n",
    "                                                                                                             train_loss.result(), \n",
    "                                                                                                             train_accuracy.result()*100, \n",
    "                                                                                                             train_top3_accuracy.result()*100))\n",
    "        if step >1000:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          train_top3_accuracy()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100,\n",
    "                          test_top3_accuracy()*100\n",
    "                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "- tf.data.TextLineDataset解析文件\n",
    "- tf.py_function 作用\n",
    "- tf.io.decode_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
